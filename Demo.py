If you donâ€™t have a GPU but have 32 GB RAM, you can still run the model locally, but with a few considerations:


---

âœ… What Changes for CPU Setup

Remove load_in_8bit=True â†’ thatâ€™s for GPUs with bitsandbytes.

Use device_map="cpu" explicitly.

Expect slower inference (CPU is much slower than GPU).

Make sure you use a smaller model (7B or below) or quantized 4-bit/8-bit versions to fit in RAM.

You can still use .safetensors safely.



---

âœ… Updated Script for CPU

Hereâ€™s the modified version of the previous script for CPU-only setup:

import os
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.vectorstores import FAISS
from sentence_transformers import SentenceTransformer
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# ==============================
# CONFIG
# ==============================
MODEL_PATH = "./models/llama"  # Change to your local model folder
CODEBASE_PATH = "./my_codebase"  # Change to your code folder
INDEX_PATH = "./code_index"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
TOP_K = 5

# ==============================
# STEP 1: Load Model on CPU
# ==============================
print("Loading model on CPU...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype="auto",
    device_map="cpu"  # CPU-only
    # Do NOT use load_in_8bit for CPU
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.2,
    do_sample=True
)
llm = HuggingFacePipeline(pipeline=pipe)

# ==============================
# STEP 2: Index Codebase (or load existing index)
# ==============================
if not os.path.exists(INDEX_PATH):
    print("Indexing codebase...")
    docs = []
    for root, dirs, files in os.walk(CODEBASE_PATH):
        for file in files:
            if file.endswith((".py", ".java", ".js", ".ts", ".html", ".css", ".md")):
                with open(os.path.join(root, file), "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    docs.append(Document(page_content=content, metadata={"source": file}))

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)

    print(f"Total chunks created: {len(chunks)}")

    # Use small embedding model
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    texts = [chunk.page_content for chunk in chunks]
    embeddings = embedder.encode(texts, convert_to_tensor=False)

    db = FAISS.from_embeddings(texts, embeddings)
    db.save_local(INDEX_PATH)
else:
    print("Loading existing index...")
    db = FAISS.load_local(INDEX_PATH, embeddings=None, allow_dangerous_deserialization=True)

retriever = db.as_retriever(search_kwargs={"k": TOP_K})

# ==============================
# STEP 3: Create QA Chain
# ==============================
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# ==============================
# STEP 4: Interactive Q&A
# ==============================
print("\nâœ… Ready! Ask questions about your codebase (type 'exit' to quit):")
while True:
    query = input("\n> ")
    if query.lower() in ["exit", "quit"]:
        break
    answer = qa.run(query)
    print(f"\nAnswer:\n{answer}\n")


---

âœ… Performance Tips for CPU

Use smaller models (e.g., 7B parameter model like LLaMA-2-7B).

Use torch_dtype="float16" if supported on CPU to save RAM:

model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype="float16", device_map="cpu")

Or quantized model in GGUF format with llama.cpp (faster for CPU than Hugging Face).

Use max_new_tokens=256 to reduce latency.



---

âœ… With 32 GB RAM, what models can you run?

LLaMA-2-7B (full precision) â†’ yes

LLaMA-2-13B â†’ maybe, but slow and memory heavy

Mistral 7B â†’ yes (recommended for better reasoning)

GPT-J, GPT-NeoX (6B) â†’ yes

Quantized versions (GGUF or GPTQ) â†’ best for CPU



---

ðŸ‘‰ Do you want me to: âœ” Give you a CPU-optimized version using llama.cpp with GGUF quantized models (much faster than Hugging Face)?
âœ” Or stick to Hugging Face but add streaming answers and memory support?
âœ” Or compare Hugging Face vs llama.cpp for your 32GB RAM scenario?

